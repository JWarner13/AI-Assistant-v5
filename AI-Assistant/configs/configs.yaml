# Ollama Configuration (Primary - Local Models)
ollama:
  llm:
    provider: "ollama"
    model: "llama3.2"  # or llama3.1, llama2, mistral, etc.
    base_url: "http://localhost:11434"
    temperature: 0.1
    max_tokens: 1024
    max_retries: 3
    timeout: 120  # seconds for local model processing
  
  embedding:
    provider: "ollama"
    ollama_model: "nomic-embed-text"  # Specialized embedding model
    base_url: "http://localhost:11434"
    dimensions: 768  # nomic-embed-text dimension
    max_retries: 3

# Fallback Configuration (SentenceTransformers)
embedding:
  provider: "sentence_transformers"  # Fallback if Ollama embeddings unavailable
  model: "all-MiniLM-L6-v2"
  dimensions: 384
  chunk_size: 1000
  max_retries: 3

# Performance Configuration
performance:
  batch:
    batch_size: 50  # Smaller batches for local models
    max_batch_size: 100
    parallel_processing: false
  
  optimization:
    enable_batch_optimization: true
    deduplicate_inputs: true
    normalize_whitespace: true
    memory_efficient: true

# Caching Configuration
cache:
  enable_response_caching: true
  enable_embedding_cache: true
  cache_ttl: 7200  # 2 hours (shorter for development)
  max_size: 500
  default_ttl: 3600

# Logging Configuration
logging:
  level: "INFO"
  file: "outputs/activity.log"
  rotation: "10 MB"
  retention: "30 days"
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name} | {message} | {extra}"
  components:
    rag_engine: "INFO"
    document_loader: "INFO"
    embedding: "INFO"
    cache: "INFO"
    reasoning: "DEBUG"
    cli: "INFO"
    performance: "INFO"
    ollama: "DEBUG"
  structured_logging: true
  performance_tracking: true
  error_aggregation: true

# Document Processing Configuration
document_processing:
  chunk_size: 500  # words
  overlap: 100     # words
  min_chunk_size: 50
  preserve_paragraphs: true
  supported_extensions: [".pdf", ".txt"]

# RAG Configuration
rag:
  retrieval:
    default_k: 5
    max_k: 10
    similarity_threshold: 0.0
    enable_reranking: false
  
  reasoning:
    enable_reasoning_trace: true
    enable_conflict_detection: true
    enable_multi_hop: true
    complexity_analysis: true

# Model Recommendations by Use Case
model_recommendations:
  # For general QA and document analysis
  general:
    llm: "llama3.2"  # Good balance of speed and quality
    embedding: "nomic-embed-text"
  
  # For faster responses (development/testing)
  fast:
    llm: "llama3.2:1b"  # Smaller, faster model
    embedding: "nomic-embed-text"
  
  # For highest quality (if you have powerful hardware)
  quality:
    llm: "llama3.1:8b"  # Larger, more capable model
    embedding: "nomic-embed-text"
  
  # For specialized domains
  code:
    llm: "deepseek-coder"
    embedding: "nomic-embed-text"

# Ollama Installation Commands (for reference)
# ollama_setup:
#   install_ollama: "curl -fsSL https://ollama.com/install.sh | sh"
#   start_service: "ollama serve"
#   install_models:
#     - "ollama pull llama3.2"
#     - "ollama pull nomic-embed-text"
#     - "ollama pull llama3.2:1b"  # Optional: smaller model
#   check_status: "ollama list"